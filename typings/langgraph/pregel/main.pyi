"""
This type stub file was generated by pyright.
"""

from collections.abc import AsyncIterator, Callable, Iterator, Mapping, Sequence
from typing import Any, Generic
from langchain_core.runnables.base import Input, Output
from langchain_core.runnables.config import RunnableConfig
from langchain_core.runnables.graph import Graph
from langgraph.cache.base import BaseCache
from langgraph.checkpoint.base import BaseCheckpointSaver
from langgraph.store.base import BaseStore
from pydantic import BaseModel
from typing_extensions import Self, Unpack, deprecated
from langgraph._internal._runnable import Runnable, RunnableLike
from langgraph._internal._typing import DeprecatedKwargs
from langgraph.channels.base import BaseChannel
from langgraph.managed.base import ManagedValueSpec
from langgraph.pregel._read import PregelNode
from langgraph.pregel._retry import RetryPolicy
from langgraph.pregel._write import ChannelWriteEntry
from langgraph.pregel.protocol import PregelProtocol
from langgraph.types import All, CachePolicy, Checkpointer, Command, Durability, StateSnapshot, StateUpdate, StreamMode
from langgraph.typing import ContextT, InputT, OutputT, StateT

__all__ = ("NodeBuilder", "Pregel")
_WriteValue = Callable[[Input], Output] | Any
class NodeBuilder:
    __slots__ = ...
    _channels: str | list[str]
    _triggers: list[str]
    _tags: list[str]
    _metadata: dict[str, Any]
    _writes: list[ChannelWriteEntry]
    _bound: Runnable
    _retry_policy: list[RetryPolicy]
    _cache_policy: CachePolicy | None
    def __init__(self) -> None:
        ...
    
    def subscribe_only(self, channel: str) -> Self:
        """Subscribe to a single channel."""
        ...
    
    def subscribe_to(self, *channels: str, read: bool = ...) -> Self:
        """Add channels to subscribe to.

        Node will be invoked when any of these channels are updated, with a dict of the
        channel values as input.

        Args:
            channels: Channel name(s) to subscribe to
            read: If `True`, the channels will be included in the input to the node.
                Otherwise, they will trigger the node without being sent in input.

        Returns:
            Self for chaining
        """
        ...
    
    def read_from(self, *channels: str) -> Self:
        """Adds the specified channels to read from, without subscribing to them."""
        ...
    
    def do(self, node: RunnableLike) -> Self:
        """Adds the specified node."""
        ...
    
    def write_to(self, *channels: str | ChannelWriteEntry, **kwargs: _WriteValue) -> Self:
        """Add channel writes.

        Args:
            *channels: Channel names to write to.
            **kwargs: Channel name and value mappings.

        Returns:
            Self for chaining
        """
        ...
    
    def meta(self, *tags: str, **metadata: Any) -> Self:
        """Add tags or metadata to the node."""
        ...
    
    def add_retry_policies(self, *policies: RetryPolicy) -> Self:
        """Adds retry policies to the node."""
        ...
    
    def add_cache_policy(self, policy: CachePolicy) -> Self:
        """Adds cache policies to the node."""
        ...
    
    def build(self) -> PregelNode:
        """Builds the node."""
        ...
    


class Pregel(PregelProtocol[StateT, ContextT, InputT, OutputT], Generic[StateT, ContextT, InputT, OutputT]):
    """Pregel manages the runtime behavior for LangGraph applications.

    ## Overview

    Pregel combines [**actors**](https://en.wikipedia.org/wiki/Actor_model)
    and **channels** into a single application.
    **Actors** read data from channels and write data to channels.
    Pregel organizes the execution of the application into multiple steps,
    following the **Pregel Algorithm**/**Bulk Synchronous Parallel** model.

    Each step consists of three phases:

    - **Plan**: Determine which **actors** to execute in this step. For example,
        in the first step, select the **actors** that subscribe to the special
        **input** channels; in subsequent steps,
        select the **actors** that subscribe to channels updated in the previous step.
    - **Execution**: Execute all selected **actors** in parallel,
        until all complete, or one fails, or a timeout is reached. During this
        phase, channel updates are invisible to actors until the next step.
    - **Update**: Update the channels with the values written by the **actors**
        in this step.

    Repeat until no **actors** are selected for execution, or a maximum number of
    steps is reached.

    ## Actors

    An **actor** is a `PregelNode`.
    It subscribes to channels, reads data from them, and writes data to them.
    It can be thought of as an **actor** in the Pregel algorithm.
    `PregelNodes` implement LangChain's
    Runnable interface.

    ## Channels

    Channels are used to communicate between actors (`PregelNodes`).
    Each channel has a value type, an update type, and an update function â€“ which
    takes a sequence of updates and
    modifies the stored value. Channels can be used to send data from one chain to
    another, or to send data from a chain to itself in a future step. LangGraph
    provides a number of built-in channels:

    ### Basic channels: LastValue and Topic

    - `LastValue`: The default channel, stores the last value sent to the channel,
       useful for input and output values, or for sending data from one step to the next
    - `Topic`: A configurable PubSub Topic, useful for sending multiple values
       between *actors*, or for accumulating output. Can be configured to deduplicate
       values, and/or to accumulate values over the course of multiple steps.

    ### Advanced channels: Context and BinaryOperatorAggregate

    - `Context`: exposes the value of a context manager, managing its lifecycle.
        Useful for accessing external resources that require setup and/or teardown. e.g.
        `client = Context(httpx.Client)`
    - `BinaryOperatorAggregate`: stores a persistent value, updated by applying
        a binary operator to the current value and each update
        sent to the channel, useful for computing aggregates over multiple steps. e.g.
        `total = BinaryOperatorAggregate(int, operator.add)`

    ## Examples

    Most users will interact with Pregel via a
    [StateGraph (Graph API)][langgraph.graph.StateGraph] or via an
    [entrypoint (Functional API)][langgraph.func.entrypoint].

    However, for **advanced** use cases, Pregel can be used directly. If you're
    not sure whether you need to use Pregel directly, then the answer is probably no
    - you should use the Graph API or Functional API instead. These are higher-level
    interfaces that will compile down to Pregel under the hood.

    Here are some examples to give you a sense of how it works:

    Example: Single node application
        ```python
        from langgraph.channels import EphemeralValue
        from langgraph.pregel import Pregel, NodeBuilder

        node1 = (
            NodeBuilder().subscribe_only("a")
            .do(lambda x: x + x)
            .write_to("b")
        )

        app = Pregel(
            nodes={"node1": node1},
            channels={
                "a": EphemeralValue(str),
                "b": EphemeralValue(str),
            },
            input_channels=["a"],
            output_channels=["b"],
        )

        app.invoke({"a": "foo"})
        ```

        ```con
        {'b': 'foofoo'}
        ```

    Example: Using multiple nodes and multiple output channels
        ```python
        from langgraph.channels import LastValue, EphemeralValue
        from langgraph.pregel import Pregel, NodeBuilder

        node1 = (
            NodeBuilder().subscribe_only("a")
            .do(lambda x: x + x)
            .write_to("b")
        )

        node2 = (
            NodeBuilder().subscribe_to("b")
            .do(lambda x: x["b"] + x["b"])
            .write_to("c")
        )


        app = Pregel(
            nodes={"node1": node1, "node2": node2},
            channels={
                "a": EphemeralValue(str),
                "b": LastValue(str),
                "c": EphemeralValue(str),
            },
            input_channels=["a"],
            output_channels=["b", "c"],
        )

        app.invoke({"a": "foo"})
        ```

        ```con
        {'b': 'foofoo', 'c': 'foofoofoofoo'}
        ```

    Example: Using a Topic channel
        ```python
        from langgraph.channels import LastValue, EphemeralValue, Topic
        from langgraph.pregel import Pregel, NodeBuilder

        node1 = (
            NodeBuilder().subscribe_only("a")
            .do(lambda x: x + x)
            .write_to("b", "c")
        )

        node2 = (
            NodeBuilder().subscribe_only("b")
            .do(lambda x: x + x)
            .write_to("c")
        )


        app = Pregel(
            nodes={"node1": node1, "node2": node2},
            channels={
                "a": EphemeralValue(str),
                "b": EphemeralValue(str),
                "c": Topic(str, accumulate=True),
            },
            input_channels=["a"],
            output_channels=["c"],
        )

        app.invoke({"a": "foo"})
        ```

        ```pycon
        {"c": ["foofoo", "foofoofoofoo"]}
        ```

    Example: Using a `BinaryOperatorAggregate` channel
        ```python
        from langgraph.channels import EphemeralValue, BinaryOperatorAggregate
        from langgraph.pregel import Pregel, NodeBuilder


        node1 = (
            NodeBuilder().subscribe_only("a")
            .do(lambda x: x + x)
            .write_to("b", "c")
        )

        node2 = (
            NodeBuilder().subscribe_only("b")
            .do(lambda x: x + x)
            .write_to("c")
        )


        def reducer(current, update):
            if current:
                return current + " | " + update
            else:
                return update


        app = Pregel(
            nodes={"node1": node1, "node2": node2},
            channels={
                "a": EphemeralValue(str),
                "b": EphemeralValue(str),
                "c": BinaryOperatorAggregate(str, operator=reducer),
            },
            input_channels=["a"],
            output_channels=["c"],
        )

        app.invoke({"a": "foo"})
        ```

        ```con
        {'c': 'foofoo | foofoofoofoo'}
        ```

    Example: Introducing a cycle
        This example demonstrates how to introduce a cycle in the graph, by having
        a chain write to a channel it subscribes to.

        Execution will continue until a `None` value is written to the channel.

        ```python
        from langgraph.channels import EphemeralValue
        from langgraph.pregel import Pregel, NodeBuilder, ChannelWriteEntry

        example_node = (
            NodeBuilder()
            .subscribe_only("value")
            .do(lambda x: x + x if len(x) < 10 else None)
            .write_to(ChannelWriteEntry(channel="value", skip_none=True))
        )

        app = Pregel(
            nodes={"example_node": example_node},
            channels={
                "value": EphemeralValue(str),
            },
            input_channels=["value"],
            output_channels=["value"],
        )

        app.invoke({"value": "a"})
        ```

        ```con
        {'value': 'aaaaaaaaaaaaaaaa'}
        ```
    """
    nodes: dict[str, PregelNode]
    channels: dict[str, BaseChannel | ManagedValueSpec]
    stream_mode: StreamMode = ...
    stream_eager: bool = ...
    output_channels: str | Sequence[str]
    stream_channels: str | Sequence[str] | None = ...
    interrupt_after_nodes: All | Sequence[str]
    interrupt_before_nodes: All | Sequence[str]
    input_channels: str | Sequence[str]
    step_timeout: float | None = ...
    debug: bool
    checkpointer: Checkpointer = ...
    store: BaseStore | None = ...
    cache: BaseCache | None = ...
    retry_policy: Sequence[RetryPolicy] = ...
    cache_policy: CachePolicy | None = ...
    context_schema: type[ContextT] | None = ...
    config: RunnableConfig | None = ...
    name: str = ...
    trigger_to_nodes: Mapping[str, Sequence[str]]
    def __init__(self, *, nodes: dict[str, PregelNode | NodeBuilder], channels: dict[str, BaseChannel | ManagedValueSpec] | None, auto_validate: bool = ..., stream_mode: StreamMode = ..., stream_eager: bool = ..., output_channels: str | Sequence[str], stream_channels: str | Sequence[str] | None = ..., interrupt_after_nodes: All | Sequence[str] = ..., interrupt_before_nodes: All | Sequence[str] = ..., input_channels: str | Sequence[str], step_timeout: float | None = ..., debug: bool | None = ..., checkpointer: BaseCheckpointSaver | None = ..., store: BaseStore | None = ..., cache: BaseCache | None = ..., retry_policy: RetryPolicy | Sequence[RetryPolicy] = ..., cache_policy: CachePolicy | None = ..., context_schema: type[ContextT] | None = ..., config: RunnableConfig | None = ..., trigger_to_nodes: Mapping[str, Sequence[str]] | None = ..., name: str = ..., **deprecated_kwargs: Unpack[DeprecatedKwargs]) -> None:
        ...
    
    def get_graph(self, config: RunnableConfig | None = ..., *, xray: int | bool = ...) -> Graph:
        """Return a drawable representation of the computation graph."""
        ...
    
    async def aget_graph(self, config: RunnableConfig | None = ..., *, xray: int | bool = ...) -> Graph:
        """Return a drawable representation of the computation graph."""
        ...
    
    def copy(self, update: dict[str, Any] | None = ...) -> Self:
        ...
    
    def with_config(self, config: RunnableConfig | None = ..., **kwargs: Any) -> Self:
        """Create a copy of the Pregel object with an updated config."""
        ...
    
    def validate(self) -> Self:
        ...
    
    @deprecated("`config_schema` is deprecated. Use `get_context_jsonschema` for the relevant schema instead.", category=None)
    def config_schema(self, *, include: Sequence[str] | None = ...) -> type[BaseModel]:
        ...
    
    @deprecated("`get_config_jsonschema` is deprecated. Use `get_context_jsonschema` instead.", category=None)
    def get_config_jsonschema(self, *, include: Sequence[str] | None = ...) -> dict[str, Any]:
        ...
    
    def get_context_jsonschema(self) -> dict[str, Any] | None:
        ...
    
    @property
    def InputType(self) -> Any:
        ...
    
    def get_input_schema(self, config: RunnableConfig | None = ...) -> type[BaseModel]:
        ...
    
    def get_input_jsonschema(self, config: RunnableConfig | None = ...) -> dict[str, Any]:
        ...
    
    @property
    def OutputType(self) -> Any:
        ...
    
    def get_output_schema(self, config: RunnableConfig | None = ...) -> type[BaseModel]:
        ...
    
    def get_output_jsonschema(self, config: RunnableConfig | None = ...) -> dict[str, Any]:
        ...
    
    @property
    def stream_channels_list(self) -> Sequence[str]:
        ...
    
    @property
    def stream_channels_asis(self) -> str | Sequence[str]:
        ...
    
    def get_subgraphs(self, *, namespace: str | None = ..., recurse: bool = ...) -> Iterator[tuple[str, PregelProtocol]]:
        """Get the subgraphs of the graph.

        Args:
            namespace: The namespace to filter the subgraphs by.
            recurse: Whether to recurse into the subgraphs.
                If `False`, only the immediate subgraphs will be returned.

        Returns:
            An iterator of the `(namespace, subgraph)` pairs.
        """
        ...
    
    async def aget_subgraphs(self, *, namespace: str | None = ..., recurse: bool = ...) -> AsyncIterator[tuple[str, PregelProtocol]]:
        """Get the subgraphs of the graph.

        Args:
            namespace: The namespace to filter the subgraphs by.
            recurse: Whether to recurse into the subgraphs.
                If `False`, only the immediate subgraphs will be returned.

        Returns:
            An iterator of the `(namespace, subgraph)` pairs.
        """
        ...
    
    def get_state(self, config: RunnableConfig, *, subgraphs: bool = ...) -> StateSnapshot:
        """Get the current state of the graph."""
        ...
    
    async def aget_state(self, config: RunnableConfig, *, subgraphs: bool = ...) -> StateSnapshot:
        """Get the current state of the graph."""
        ...
    
    def get_state_history(self, config: RunnableConfig, *, filter: dict[str, Any] | None = ..., before: RunnableConfig | None = ..., limit: int | None = ...) -> Iterator[StateSnapshot]:
        """Get the history of the state of the graph."""
        ...
    
    async def aget_state_history(self, config: RunnableConfig, *, filter: dict[str, Any] | None = ..., before: RunnableConfig | None = ..., limit: int | None = ...) -> AsyncIterator[StateSnapshot]:
        """Asynchronously get the history of the state of the graph."""
        ...
    
    def bulk_update_state(self, config: RunnableConfig, supersteps: Sequence[Sequence[StateUpdate]]) -> RunnableConfig:
        """Apply updates to the graph state in bulk. Requires a checkpointer to be set.

        Args:
            config: The config to apply the updates to.
            supersteps: A list of supersteps, each including a list of updates to apply sequentially to a graph state.

                Each update is a tuple of the form `(values, as_node, task_id)` where `task_id` is optional.

        Raises:
            ValueError: If no checkpointer is set or no updates are provided.
            InvalidUpdateError: If an invalid update is provided.

        Returns:
            RunnableConfig: The updated config.
        """
        ...
    
    async def abulk_update_state(self, config: RunnableConfig, supersteps: Sequence[Sequence[StateUpdate]]) -> RunnableConfig:
        """Asynchronously apply updates to the graph state in bulk. Requires a checkpointer to be set.

        Args:
            config: The config to apply the updates to.
            supersteps: A list of supersteps, each including a list of updates to apply sequentially to a graph state.

                Each update is a tuple of the form `(values, as_node, task_id)` where `task_id` is optional.

        Raises:
            ValueError: If no checkpointer is set or no updates are provided.
            InvalidUpdateError: If an invalid update is provided.

        Returns:
            RunnableConfig: The updated config.
        """
        ...
    
    def update_state(self, config: RunnableConfig, values: dict[str, Any] | Any | None, as_node: str | None = ..., task_id: str | None = ...) -> RunnableConfig:
        """Update the state of the graph with the given values, as if they came from
        node `as_node`. If `as_node` is not provided, it will be set to the last node
        that updated the state, if not ambiguous.
        """
        ...
    
    async def aupdate_state(self, config: RunnableConfig, values: dict[str, Any] | Any, as_node: str | None = ..., task_id: str | None = ...) -> RunnableConfig:
        """Asynchronously update the state of the graph with the given values, as if they came from
        node `as_node`. If `as_node` is not provided, it will be set to the last node
        that updated the state, if not ambiguous.
        """
        ...
    
    def stream(self, input: InputT | Command | None, config: RunnableConfig | None = ..., *, context: ContextT | None = ..., stream_mode: StreamMode | Sequence[StreamMode] | None = ..., print_mode: StreamMode | Sequence[StreamMode] = ..., output_keys: str | Sequence[str] | None = ..., interrupt_before: All | Sequence[str] | None = ..., interrupt_after: All | Sequence[str] | None = ..., durability: Durability | None = ..., subgraphs: bool = ..., debug: bool | None = ..., **kwargs: Unpack[DeprecatedKwargs]) -> Iterator[dict[str, Any] | Any]:
        """Stream graph steps for a single input.

        Args:
            input: The input to the graph.
            config: The configuration to use for the run.
            context: The static context to use for the run.
                !!! version-added "Added in version 0.6.0"
            stream_mode: The mode to stream output, defaults to `self.stream_mode`.

                Options are:

                - `"values"`: Emit all values in the state after each step, including interrupts.
                    When used with functional API, values are emitted once at the end of the workflow.
                - `"updates"`: Emit only the node or task names and updates returned by the nodes or tasks after each step.
                    If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are emitted separately.
                - `"custom"`: Emit custom data from inside nodes or tasks using `StreamWriter`.
                - `"messages"`: Emit LLM messages token-by-token together with metadata for any LLM invocations inside nodes or tasks.
                    - Will be emitted as 2-tuples `(LLM token, metadata)`.
                - `"checkpoints"`: Emit an event when a checkpoint is created, in the same format as returned by `get_state()`.
                - `"tasks"`: Emit events when tasks start and finish, including their results and errors.
                - `"debug"`: Emit debug events with as much information as possible for each step.

                You can pass a list as the `stream_mode` parameter to stream multiple modes at once.
                The streamed outputs will be tuples of `(mode, data)`.

                See [LangGraph streaming guide](https://docs.langchain.com/oss/python/langgraph/streaming) for more details.
            print_mode: Accepts the same values as `stream_mode`, but only prints the output to the console, for debugging purposes.

                Does not affect the output of the graph in any way.
            output_keys: The keys to stream, defaults to all non-context channels.
            interrupt_before: Nodes to interrupt before, defaults to all nodes in the graph.
            interrupt_after: Nodes to interrupt after, defaults to all nodes in the graph.
            durability: The durability mode for the graph execution, defaults to `"async"`.

                Options are:

                - `"sync"`: Changes are persisted synchronously before the next step starts.
                - `"async"`: Changes are persisted asynchronously while the next step executes.
                - `"exit"`: Changes are persisted only when the graph exits.
            subgraphs: Whether to stream events from inside subgraphs, defaults to `False`.

                If `True`, the events will be emitted as tuples `(namespace, data)`,
                or `(namespace, mode, data)` if `stream_mode` is a list,
                where `namespace` is a tuple with the path to the node where a subgraph is invoked,
                e.g. `("parent_node:<task_id>", "child_node:<task_id>")`.

                See [LangGraph streaming guide](https://docs.langchain.com/oss/python/langgraph/streaming) for more details.

        Yields:
            The output of each step in the graph. The output shape depends on the `stream_mode`.
        """
        ...
    
    async def astream(self, input: InputT | Command | None, config: RunnableConfig | None = ..., *, context: ContextT | None = ..., stream_mode: StreamMode | Sequence[StreamMode] | None = ..., print_mode: StreamMode | Sequence[StreamMode] = ..., output_keys: str | Sequence[str] | None = ..., interrupt_before: All | Sequence[str] | None = ..., interrupt_after: All | Sequence[str] | None = ..., durability: Durability | None = ..., subgraphs: bool = ..., debug: bool | None = ..., **kwargs: Unpack[DeprecatedKwargs]) -> AsyncIterator[dict[str, Any] | Any]:
        """Asynchronously stream graph steps for a single input.

        Args:
            input: The input to the graph.
            config: The configuration to use for the run.
            context: The static context to use for the run.
                !!! version-added "Added in version 0.6.0"
            stream_mode: The mode to stream output, defaults to `self.stream_mode`.

                Options are:

                - `"values"`: Emit all values in the state after each step, including interrupts.
                    When used with functional API, values are emitted once at the end of the workflow.
                - `"updates"`: Emit only the node or task names and updates returned by the nodes or tasks after each step.
                    If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are emitted separately.
                - `"custom"`: Emit custom data from inside nodes or tasks using `StreamWriter`.
                - `"messages"`: Emit LLM messages token-by-token together with metadata for any LLM invocations inside nodes or tasks.
                    - Will be emitted as 2-tuples `(LLM token, metadata)`.
                - `"checkpoints"`: Emit an event when a checkpoint is created, in the same format as returned by `get_state()`.
                - `"tasks"`: Emit events when tasks start and finish, including their results and errors.
                - `"debug"`: Emit debug events with as much information as possible for each step.

                You can pass a list as the `stream_mode` parameter to stream multiple modes at once.
                The streamed outputs will be tuples of `(mode, data)`.

                See [LangGraph streaming guide](https://docs.langchain.com/oss/python/langgraph/streaming) for more details.
            print_mode: Accepts the same values as `stream_mode`, but only prints the output to the console, for debugging purposes.

                Does not affect the output of the graph in any way.
            output_keys: The keys to stream, defaults to all non-context channels.
            interrupt_before: Nodes to interrupt before, defaults to all nodes in the graph.
            interrupt_after: Nodes to interrupt after, defaults to all nodes in the graph.
            durability: The durability mode for the graph execution, defaults to `"async"`.

                Options are:

                - `"sync"`: Changes are persisted synchronously before the next step starts.
                - `"async"`: Changes are persisted asynchronously while the next step executes.
                - `"exit"`: Changes are persisted only when the graph exits.
            subgraphs: Whether to stream events from inside subgraphs, defaults to `False`.

                If `True`, the events will be emitted as tuples `(namespace, data)`,
                or `(namespace, mode, data)` if `stream_mode` is a list,
                where `namespace` is a tuple with the path to the node where a subgraph is invoked,
                e.g. `("parent_node:<task_id>", "child_node:<task_id>")`.

                See [LangGraph streaming guide](https://docs.langchain.com/oss/python/langgraph/streaming) for more details.

        Yields:
            The output of each step in the graph. The output shape depends on the `stream_mode`.
        """
        ...
    
    def invoke(self, input: InputT | Command | None, config: RunnableConfig | None = ..., *, context: ContextT | None = ..., stream_mode: StreamMode = ..., print_mode: StreamMode | Sequence[StreamMode] = ..., output_keys: str | Sequence[str] | None = ..., interrupt_before: All | Sequence[str] | None = ..., interrupt_after: All | Sequence[str] | None = ..., durability: Durability | None = ..., **kwargs: Any) -> dict[str, Any] | Any:
        """Run the graph with a single input and config.

        Args:
            input: The input data for the graph. It can be a dictionary or any other type.
            config: The configuration for the graph run.
            context: The static context to use for the run.
                !!! version-added "Added in version 0.6.0"
            stream_mode: The stream mode for the graph run.
            print_mode: Accepts the same values as `stream_mode`, but only prints the output to the console, for debugging purposes.

                Does not affect the output of the graph in any way.
            output_keys: The output keys to retrieve from the graph run.
            interrupt_before: The nodes to interrupt the graph run before.
            interrupt_after: The nodes to interrupt the graph run after.
            durability: The durability mode for the graph execution, defaults to `"async"`.

                Options are:

                - `"sync"`: Changes are persisted synchronously before the next step starts.
                - `"async"`: Changes are persisted asynchronously while the next step executes.
                - `"exit"`: Changes are persisted only when the graph exits.
            **kwargs: Additional keyword arguments to pass to the graph run.

        Returns:
            The output of the graph run. If `stream_mode` is `"values"`, it returns the latest output.
            If `stream_mode` is not `"values"`, it returns a list of output chunks.
        """
        ...
    
    async def ainvoke(self, input: InputT | Command | None, config: RunnableConfig | None = ..., *, context: ContextT | None = ..., stream_mode: StreamMode = ..., print_mode: StreamMode | Sequence[StreamMode] = ..., output_keys: str | Sequence[str] | None = ..., interrupt_before: All | Sequence[str] | None = ..., interrupt_after: All | Sequence[str] | None = ..., durability: Durability | None = ..., **kwargs: Any) -> dict[str, Any] | Any:
        """Asynchronously run the graph with a single input and config.

        Args:
            input: The input data for the graph. It can be a dictionary or any other type.
            config: The configuration for the graph run.
            context: The static context to use for the run.
                !!! version-added "Added in version 0.6.0"
            stream_mode: The stream mode for the graph run.
            print_mode: Accepts the same values as `stream_mode`, but only prints the output to the console, for debugging purposes.

                Does not affect the output of the graph in any way.
            output_keys: The output keys to retrieve from the graph run.
            interrupt_before: The nodes to interrupt the graph run before.
            interrupt_after: The nodes to interrupt the graph run after.
            durability: The durability mode for the graph execution, defaults to `"async"`.

                Options are:

                - `"sync"`: Changes are persisted synchronously before the next step starts.
                - `"async"`: Changes are persisted asynchronously while the next step executes.
                - `"exit"`: Changes are persisted only when the graph exits.
            **kwargs: Additional keyword arguments to pass to the graph run.

        Returns:
            The output of the graph run. If `stream_mode` is `"values"`, it returns the latest output.
            If `stream_mode` is not `"values"`, it returns a list of output chunks.
        """
        ...
    
    def clear_cache(self, nodes: Sequence[str] | None = ...) -> None:
        """Clear the cache for the given nodes."""
        ...
    
    async def aclear_cache(self, nodes: Sequence[str] | None = ...) -> None:
        """Asynchronously clear the cache for the given nodes."""
        ...
    


