"""
This type stub file was generated by pyright.
"""

from collections.abc import AsyncIterator, Callable, Iterator
from typing import Any, TypeVar
from uuid import UUID
from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.messages import BaseMessage
from langchain_core.outputs import ChatGenerationChunk, LLMResult
from langgraph.pregel.protocol import StreamChunk
from langchain_core.tracers._streaming import _StreamingCallbackHandler

T = TypeVar("T")
Meta = tuple[tuple[str, ...], dict[str, Any]]
class StreamMessagesHandler(BaseCallbackHandler, _StreamingCallbackHandler):
    """A callback handler that implements stream_mode=messages.

    Collects messages from:
    (1) chat model stream events; and
    (2) node outputs.
    """
    run_inline = ...
    def __init__(self, stream: Callable[[StreamChunk], None], subgraphs: bool, *, parent_ns: tuple[str, ...] | None = ...) -> None:
        """Configure the handler to stream messages from LLMs and nodes.

        Args:
            stream: A callable that takes a StreamChunk and emits it.
            subgraphs: Whether to emit messages from subgraphs.
            parent_ns: The namespace where the handler was created.
                We keep track of this namespace to allow calls to subgraphs that
                were explicitly requested as a stream with `messages` mode
                configured.

        Example:
            parent_ns is used to handle scenarios where the subgraph is explicitly
            streamed with `stream_mode="messages"`.

            ```python
            def parent_graph_node():
                # This node is in the parent graph.
                async for event in some_subgraph(..., stream_mode="messages"):
                    do something with event # <-- these events will be emitted
                return ...

            parent_graph.invoke(subgraphs=False)
            ```
        """
        ...
    
    def tap_output_aiter(self, run_id: UUID, output: AsyncIterator[T]) -> AsyncIterator[T]:
        ...
    
    def tap_output_iter(self, run_id: UUID, output: Iterator[T]) -> Iterator[T]:
        ...
    
    def on_chat_model_start(self, serialized: dict[str, Any], messages: list[list[BaseMessage]], *, run_id: UUID, parent_run_id: UUID | None = ..., tags: list[str] | None = ..., metadata: dict[str, Any] | None = ..., **kwargs: Any) -> Any:
        ...
    
    def on_llm_new_token(self, token: str, *, chunk: ChatGenerationChunk | None = ..., run_id: UUID, parent_run_id: UUID | None = ..., tags: list[str] | None = ..., **kwargs: Any) -> Any:
        ...
    
    def on_llm_end(self, response: LLMResult, *, run_id: UUID, parent_run_id: UUID | None = ..., **kwargs: Any) -> Any:
        ...
    
    def on_llm_error(self, error: BaseException, *, run_id: UUID, parent_run_id: UUID | None = ..., **kwargs: Any) -> Any:
        ...
    
    def on_chain_start(self, serialized: dict[str, Any], inputs: dict[str, Any], *, run_id: UUID, parent_run_id: UUID | None = ..., tags: list[str] | None = ..., metadata: dict[str, Any] | None = ..., **kwargs: Any) -> Any:
        ...
    
    def on_chain_end(self, response: Any, *, run_id: UUID, parent_run_id: UUID | None = ..., **kwargs: Any) -> Any:
        ...
    
    def on_chain_error(self, error: BaseException, *, run_id: UUID, parent_run_id: UUID | None = ..., **kwargs: Any) -> Any:
        ...
    


